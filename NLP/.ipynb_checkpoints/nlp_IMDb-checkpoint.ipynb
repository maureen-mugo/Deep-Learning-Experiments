{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9a6b58-b33a-41e9-a645-b192d23d9561",
   "metadata": {},
   "source": [
    "# NLP with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ad72b-469b-4799-981b-fe2f0bd0e068",
   "metadata": {},
   "source": [
    "This notebook uses RNN to do text classification. We use the IMDB dataset to train a model that classifies movie reviews as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaab64a-3b82-4866-9f34-7372474384fe",
   "metadata": {},
   "source": [
    "## Tokenization  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb467d9-53b9-4ccc-bb7a-997fdf1f98ef",
   "metadata": {},
   "source": [
    "### Word Tokenization with fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb1398-9d75-40de-a0a4-c3fdf6d78177",
   "metadata": {},
   "source": [
    "Grabbing the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc253e-91b8-4cb6-90a9-49679b96d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3bf0d-3d7e-4da2-ae7d-1edab1a8799e",
   "metadata": {},
   "source": [
    "To get the text file in the path for tokenization using get_text_file.  \n",
    "We cna also pass the folders to restrict the search to a particular list of subfolders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf143-f788-430b-9831-8544d1ddad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dca851-b1a6-4bdd-a7c8-3f2c5b18257a",
   "metadata": {},
   "source": [
    "Grabbing the first file, open and read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021e0d3-ecc6-4c01-bba9-50e8f5dc1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ab163-3651-43ab-801e-36840bd97da8",
   "metadata": {},
   "source": [
    "Fastai uses a library called spaCy for tokenization. As we are doing word tokenization, we will have to specify that.  \n",
    "Also, we use fastai's coll_repr(collection, n) function to display the results. This displays the first n items of collection.  \n",
    "We have to pass txt as a list to our tokenizer(spacy) as it only takes a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e772a7-d4e5-4ecc-b120-355fc6f7476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d97af-991b-4eed-8874-36c2fcb48a33",
   "metadata": {},
   "source": [
    "spaCy separates \".\" when it's being used to terminate a sentence but not in an acroynm or number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaec474-d2bd-4d12-a5bd-d52a039339c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e9bb2-5e71-4bcf-a59c-650d4b4550f9",
   "metadata": {},
   "source": [
    "Tokenizer class by fastai adds some additional functionality to the tokenization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15268910-d258-4a4e-a028-48e3ca2dc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2d96e-b64c-446f-b70c-f89dbd6127ac",
   "metadata": {},
   "source": [
    "This allows us to lowercase everything so that the embedding matrix can only work with lowercase text. \n",
    "Words that begin with a capital letter have a special token 'xxmaj' while the beginning of a stream is indicated by 'xxbos'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383b81d-b2b2-43a3-881b-b562a7899e66",
   "metadata": {},
   "source": [
    "Other special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287f246-4dec-4305-9ff6-3a12adc08fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c373dbc-4159-4722-abba-642ca14a7716",
   "metadata": {},
   "source": [
    "xxrep: replaces any character repeated 3 or more times with a special token with a special token for repetition (xxrep), the number of times it's repeated, then the character.  \n",
    "xxup: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb9fce-bb00-48f3-8638-475a06c21b14",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cef4ee-1627-44d9-9f0c-496f17090f95",
   "metadata": {},
   "source": [
    "This method follows the following steps:  \n",
    "1) Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.\n",
    "2) Tokenize the corpus using this vocab of subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff8b90-909d-41cb-afea-0a68b546328e",
   "metadata": {},
   "source": [
    "Let's look at an example.  \n",
    "For our corpus, we'll use the first 2,000 movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1143b-9250-4308-a1ae-1bb7b3130650",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in [:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0442e-a9c1-476d-b1f5-895975f976fe",
   "metadata": {},
   "source": [
    "Now we can use setup(), which is a special fastai method that is used to train our Tokenizer to find common sequences of characters to create the vocab.  \n",
    "We'll create a function that takes a certain size of a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc614b-71a2-4a86-962d-91ac1dd39bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubWordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp[txts]))[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34629ff-00ca-4680-900f-767ab41ca6d0",
   "metadata": {},
   "source": [
    "Trying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc68fa9-eff4-4984-94ee-32703522356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120bac0-71c7-4deb-a4a7-b54554bd5033",
   "metadata": {},
   "source": [
    "The special character ‚ñÅ represents a space character in the original text when using fastai's subword tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e97f5-8971-4c9c-b86f-08c188d4f7da",
   "metadata": {},
   "source": [
    "For small vocabs, each token will represent fewer characters, and it will take more tokens to represent a sentence.  \n",
    "For larger vocabs, most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa542f13-b439-40cc-ba38-ebdf3c4f1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b04a64-bc0f-4116-bb9f-a513e53add6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceee541-83df-4b2b-8911-d9d48c8f8ac1",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233dcdb-b6ac-4729-a434-8c40eec8cd6b",
   "metadata": {},
   "source": [
    "Numericalization is the process of mapping tokens to integers. It involves:  \n",
    "1) Make a list of all possible levels of that categorical variable (the vocab).  \n",
    "2) Replace each level with its index in the vocab. \n",
    "\n",
    "We'll use the word tokenized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e208d-b21f-4c06-a439-0621d90f9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405e410-db46-4d0d-b43c-303d832a0122",
   "metadata": {},
   "source": [
    "In order to numericalize, we first have to call setup() that creates a vocab.  \n",
    "Since tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ce388-7986-4990-a193-39b2123856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6064c-e81b-473c-90f5-8e7db61d4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505cbdb-7e4a-48f8-9068-b22d07ad1bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
