{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ce7a7e-df56-498b-bf6c-2074580def72",
   "metadata": {},
   "source": [
    "# Language Models from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7b1d5-8bb1-450f-911d-e83c23148962",
   "metadata": {},
   "source": [
    "### Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a5775-ca61-4dbe-8211-095803fa70bf",
   "metadata": {},
   "source": [
    "The dataset contains the first 10,000 numbers written out in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b79d3-e368-4557-9e7d-d77912132269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebc504-9324-4242-aeff-a4bd8eda1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.HUMAN_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebca700-e498-4922-9239-2b2647471641",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d26842-3b09-448a-900d-55b3944b0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e179f3-36cb-45d6-baaa-fd07edf1f705",
   "metadata": {},
   "source": [
    "Let's open those two files and see what's inside. At first we'll join all of the texts together and ignore the train/valid split given by the dataset (we'll come back to that later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea46c25-31b1-4204-8eb4-0cfcbeb35ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = L()\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
    "lenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5d542-8110-43b8-b4d4-6ee811fde696",
   "metadata": {},
   "source": [
    "We take all those lines and concatenate them in one big stream. To mark when we go from one number to the next, we use a '.' as a separator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180c2d1-a306-4bf3-a39b-62ad92a6c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' . '.join([l.strip() for l in lines])\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2caa6-70cd-4764-b8dd-aa92372d85b1",
   "metadata": {},
   "source": [
    "We can tokenize this dataset by splitting on spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe4e17-10e8-4c96-ab40-5c6b85f29925",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split(' ')\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb1716-672e-4289-bad2-8263c4043f3c",
   "metadata": {},
   "source": [
    "To numericalize, we have to create a list of all the unique tokens (our vocab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52929533-50a1-4118-b648-8e9d220c4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = L(*tokens).unique()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fba84-2b3f-4fe1-853a-bb335e6753e8",
   "metadata": {},
   "source": [
    "Then we can convert our tokens into numbers by looking up the index of each in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a629ed-8f7d-4fc2-a710-e70faa83f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w:x for i,w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e842b44-4afa-499a-9481-aadcf137e670",
   "metadata": {},
   "source": [
    "Now we have a small dataset, we can create a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d16856-0f3b-46eb-a84f-73dfbdc82377",
   "metadata": {},
   "source": [
    "## Language Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ff305-658b-4634-8caf-8e6d19aa31a3",
   "metadata": {},
   "source": [
    "To turn this into a neural network would be to specify that we are going to predict each word based on the previous three words.  \n",
    "We can do that with plain Python. First, we need to go through our token and create a range from 0 to the length of our tokens minus 4, and every 3 of them. That will allow us to grab 3 tokens at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1d688-0307-4150-9858-1f71963f4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f377c3-99aa-48ca-ba3b-9eeaa3f4817e",
   "metadata": {},
   "source": [
    "Now we will do it with tensors of the numericalized values, which is what the model will actually use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1da64-7c1c-4709-aea0-579aa78fe03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3))\n",
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8c137-98be-4539-b67c-e172a7311ed8",
   "metadata": {},
   "source": [
    "We can create batches using dataloaders and split them randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bdfd9-10a0-4710-9236-0af07bf2374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd7dd0-b0d9-417f-855c-02dbe25b47fe",
   "metadata": {},
   "source": [
    "We can now create a neural network architecture that takes three numericalized words as input and tries to predict the next as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295dbda-edd7-4ed1-a55d-72c72e9f53d3",
   "metadata": {},
   "source": [
    "### Our Language Model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d2dca-0e18-480c-91de-8599b2c9dc2b",
   "metadata": {},
   "source": [
    "We can now create the language model module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206accc-f3e5-4e56-b6ce-5dcc77eaeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.h_h(self.i_h(x[:,0])))\n",
    "        h = h + self.i_h(x[:,1])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        h = h + self.i_h(x[:,2])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7af40-3cf0-47bf-910d-62c80e2167ff",
   "metadata": {},
   "source": [
    "This is a three layer neural network.  \n",
    "- The embedding layer (i_h, for input to hidden)\n",
    "- The linear layer to create the activations for the next word (h_h, for hidden to hidden)\n",
    "- A final linear layer to predict the fourth word (h_o, for hidden to output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75f1c8-f419-49dd-9ad1-942a11f2c170",
   "metadata": {},
   "source": [
    "Let's try training this model and see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705cbb9-258f-4e3f-9e9a-d47153b9ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3eb2a3-248c-4a5d-bb2b-b987df75d1e7",
   "metadata": {},
   "source": [
    "To see if this is any good, let's check what a very simple model would give us. In this case we could always predict the most common token, so let's find out which token is most often the target in our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b69fa7-c6b0-4cf5-a9f2-71d0ed55bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,counts = 0,torch.zeros(len(vocab))\n",
    "for x,y in dls.valid:\n",
    "    n += y.shape[0]\n",
    "    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
    "idx = torch.argmax(counts)\n",
    "idx, vocab[idx.item()], counts[idx].item()/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac64d61-998a-4a01-907c-2d7f27a2d9e6",
   "metadata": {},
   "source": [
    "The most common token has the index 29, which corresponds to the token `thousand`. Always predicting this token would give us an accuracy of roughly 15\\%, while our model got an accuracy close to 50% making it better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a751f-a497-4ab4-9e10-80d310fcdaac",
   "metadata": {},
   "source": [
    "### Our First Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732c102-ec36-4dd7-8e27-426b316b3e86",
   "metadata": {},
   "source": [
    "Simplifying our code with the 'for' loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f9944-6324-49ad-8e31-ba07375e1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = 0\n",
    "        for i in range(3):\n",
    "            h = h + self.i_h(x[:,i])\n",
    "            h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b24eb-ba8b-4a72-8d46-35451f639e32",
   "metadata": {},
   "source": [
    "Let's check that we get the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a291b7-739e-45ff-8ed9-960050c1b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc922d-be74-408c-8d89-e9fa1fa38c2f",
   "metadata": {},
   "source": [
    "A neural network that is defined using a loop like this is called a recurrent neural network (RNN). It is simply a refactoring of a multilayer neural network using a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68e4dc-9ac1-4a21-84aa-81ce7f357cfd",
   "metadata": {},
   "source": [
    "### Improving our RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfba928-5129-4db6-87c3-a03569a19b43",
   "metadata": {},
   "source": [
    "Looking at the code for our RNN, one thing that seems problematic is that we are initializing our hidden state to zero for every new input sequence. Why is that a problem? 0 plus a tensor would broadcast 0. For every batch we are setting our hidden state to 0 even though we are going through the entire set of human numbers dataset in order. Thus, you would think by the time you go through like 1,2,3 you shouldn't then forget everything we've learnt when going through 4,5,6. It would be great to remember where we are upto and not reset the hidden state every time. Thus, we need to maintain the state of the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e71b7b-caad-4efe-beb5-e85da4d208ad",
   "metadata": {},
   "source": [
    "#### Maintaining the state of the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf18df7-e25c-4512-9220-8189dc0ac170",
   "metadata": {},
   "source": [
    "Because we initialize the model's hidden state to zero for each new sample, we are throwing away all the information we have about the sentences we have seen so far, which means that our model doesn't actually know where we are up to in the overall counting sequence. This is easily fixed; we can simply move the initialization of the hidden state to __init__.\n",
    "\n",
    "But this fix will create its own subtle, but important, problem. It effectively makes our neural network as deep as the entire number of tokens in our document. For instance, if there were 10,000 tokens in our dataset, we would be creating a 10,000-layer neural network.\n",
    "\n",
    "To see why this is the case, consider the original pictorial representation of our recurrent neural network in <>, before refactoring it with a for loop. You can see each layer corresponds with one token input. When we talk about the representation of a recurrent neural network before refactoring with the for loop, we call this the unrolled representation. It is often helpful to consider the unrolled representation when trying to understand an RNN.\n",
    "\n",
    "The problem with a 10,000-layer neural network is that if and when you get to the 10,000th word of the dataset, you will still need to calculate the derivatives all the way back to the first layer. This is going to be very slow indeed, and very memory-intensive. It is unlikely that you'll be able to store even one mini-batch on your GPU.\n",
    "\n",
    "The solution to this problem is to tell PyTorch that we do not want to back propagate the derivatives through the entire implicit neural network. Instead, we will just keep the last three layers of gradients. To remove all of the gradient history in PyTorch, we use the detach method.\n",
    "\n",
    "Here is the new version of our RNN. It is now stateful, because it remembers its activations between different calls to forward, which represent its use for different samples in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee300da4-7cb0-4a5f-855b-9dc732e3db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(3):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "        out = self.h_o(self.h)\n",
    "        self.h = self.h.detach()\n",
    "        return out\n",
    "    \n",
    "    def reset(self): self.h = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8ffa0-7b01-43b5-ba76-483ecd781ae3",
   "metadata": {},
   "source": [
    "This model will have the same activations whatever sequence length we pick, because the hidden state will remember the last activation from the previous batch. The only thing that will be different is the gradients computed at each step: they will only be calculated on sequence length tokens in the past, instead of the whole stream. This approach is called backpropagation through time (BPTT).  \n",
    "Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which \"detaches\" the history of computation steps in the hidden state every few time steps.  \n",
    "To use LMModel3, we need to make sure the samples are going to be seen in a certain order. As we saw in previously, if the first line of the first batch is our dset[0] then the second batch should have dset[1] as the first line, so that the model sees the text flowing.  \n",
    "To do this, we are going to rearrange our dataset. First we divide the samples into m = len(dset) // bs groups (this is the equivalent of splitting the whole concatenated dataset into, for example, 64 equally sized pieces, since we're using bs=64 here). m is the length of each of these pieces. For instance, if we're using our whole dataset (although we'll actually split it into train versus valid in a moment), that will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13efaf-d8fc-47e8-87bd-1c775f700079",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(seqs)//bs\n",
    "m,bs,len(seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a27fcf-10a1-4cc0-9fd2-6fd77bfb98dc",
   "metadata": {},
   "source": [
    "This way, at each epoch, the model will see a chunk of contiguous text of size 3*m (since each text is of size 3) on each line of the batch.\n",
    "\n",
    "The following function does that reindexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85974515-92f4-4a32-90d7-d2a7ce82a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c848261-6f05-4554-b646-32e11a8ad800",
   "metadata": {},
   "source": [
    "Then we just pass drop_last=True when building our DataLoaders to drop the last batch that does not have a shape of bs. We also pass shuffle=False to make sure the texts are read in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc6acd-8a21-43bd-80e4-8ed9f2142f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], bs), \n",
    "    group_chunks(seqs[cut:], bs), \n",
    "    bs=bs, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff67fd-0fc1-43b4-90e1-b9140a51b22a",
   "metadata": {},
   "source": [
    "Now we need to add CallBack to our learner. This one will call the reset method of our model at the beginning of each epoch and before each validation phase. Since we implemented that method to zero the hidden state of the model, this will make sure we start with a clean state before reading those continuous chunks of text. We can also start training a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef432395-09d8-415c-a1e6-2bf6bb48d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(10, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce98a44-6275-4919-bef1-af099496d61d",
   "metadata": {},
   "source": [
    "### Creating More Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3cbcc-fe1f-4a32-9feb-b4208554f52f",
   "metadata": {},
   "source": [
    "Another problem with our current approach is that we only predict one output word for each three input words. That means that the amount of signal that we are feeding back to update weights with is not as large as it could be. It would be better if we predicted the next word after every single word, rather than every three words.  \n",
    "This is easy enough to add. We need to first change our data so that the dependent variable has each of the three next words after each of our three input words. Instead of 3, we use an attribute, sl (for sequence length), and make it a bit bigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868369e-6d6d-4eac-8a5a-481df6293237",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 16\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0,len(nums)-sl-1,sl))\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
    "                             group_chunks(seqs[cut:], bs),\n",
    "                             bs=bs, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9391b9e-c2d5-4ca0-8214-c78376a1c3dd",
   "metadata": {},
   "source": [
    "Looking at the first element of `seqs`, we can see that it contains two lists of the same size. The second list is the same as the first, but offset by one element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533a078-8a4b-41b6-b33a-a40e5925397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[L(vocab[o] for o in s) for s in seqs[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8919d-0dd0-4c70-9511-85e8b659e667",
   "metadata": {},
   "source": [
    "Now we need to modify our model so that it outputs a prediction after every word, rather than just at the end of a three-word sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87865289-3336-4a98-a5c2-e0068c145a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel4(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for i in range(sl):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "            outs.append(self.h_o(self.h))\n",
    "        self.h = self.h.detach()\n",
    "        return torch.stack(outs, dim=1)\n",
    "    \n",
    "    def reset(self): self.h = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a56f17-b8ed-4090-bd5a-724fe19b6429",
   "metadata": {},
   "source": [
    "This model will return outputs of shape bs x sl x vocab_sz (since we stacked on dim=1). Our targets are of shape bs x sl, so we need to flatten those before using them in F.cross_entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a8c9a-117d-44b8-a9c3-e8283e72f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a815c9-1612-4a4c-8471-af1ce9a1f68b",
   "metadata": {},
   "source": [
    "We can now train the model using our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d55f9-ff9f-4522-8ef0-074299165669",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbc385-9cd0-416c-9589-8492fcff7702",
   "metadata": {},
   "source": [
    "This gives us better accuracy. Now, the obvious way to get a better model is to go deeper: we only have one linear layer between the hidden state and the output activations in our basic RNN, so maybe we'll get better results with more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241e4cb-a569-4dd6-9005-b55b9ac663a3",
   "metadata": {},
   "source": [
    "### Multilayer RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f34231-1be6-471b-bf74-c45c595ab795",
   "metadata": {},
   "source": [
    "In a multilayer RNN, we pass the activations from our recurrent neural network into a second recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556ec84-6eca-447b-a586-fdffa12819a2",
   "metadata": {},
   "source": [
    "#### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91217b2d-c3db-457c-8a15-7a331c6711c2",
   "metadata": {},
   "source": [
    "We can use PyTorch's RNN class, which implements exactly what we created earlier, but also gives us the option to stack multiple RNNs(using n_layers parameter), as we have discussed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221c5cf-e0ef-4b5c-90fd-82f04be00e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = h.detach()\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): self.h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a82eac-189e-4867-a50f-b653aa043522",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140b46c-e410-44d1-9871-c13fd8ead278",
   "metadata": {},
   "source": [
    "The accuracy reduces as compared to the single layer RNN model. This is because we have a deeper model, leading to exploding or vanishing activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b6749-00d7-46ae-9923-60ca57d0df86",
   "metadata": {},
   "source": [
    "### Exploding or Disappearing Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ec1f9-1784-4f16-96ef-2c54b07a83c5",
   "metadata": {},
   "source": [
    "Exploding and disappearing activations often occur due to the matrix multiplication that occurs during model training. Think about what happens when you multiply by a number many times. For example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,... after 32 steps you are already at 4,294,967,296. A similar issue happens if you multiply by 0.5: you get 0.5, 0.25, 0.125… and after 32 steps it's 0.00000000023. As you can see, multiplying by a number even slightly higher or lower than 1 results in an explosion or disappearance of our starting number, after just a few repeated multiplications.\n",
    "\n",
    "Because matrix multiplication is just multiplying numbers and adding them up, exactly the same thing happens with repeated matrix multiplications. And that's all a deep neural network is —each extra layer is another matrix multiplication. This means that it is very easy for a deep neural network to end up with extremely large or extremely small numbers.\n",
    "\n",
    "This inaccuracy means that often the gradients calculated for updating the weights end up as zero or infinity for deep networks. This is commonly referred to as the vanishing gradients or exploding gradients problem. It means that in SGD, the weights are either not updated at all or jump to infinity. Either way, they won't improve with training.  \n",
    "\n",
    "For RNNs, there are two types of layers that are frequently used to avoid exploding activations: gated recurrent units (GRUs) and long short-term memory (LSTM) layers. Both of these are available in PyTorch, and are drop-in replacements for the RNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e487e81-afa0-4b9a-b434-e074d56c5f75",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec0d0c-e261-404b-85a8-13ef1c3ffda1",
   "metadata": {},
   "source": [
    "LSTM is an architecture with not one but two hidden state. In our base RNN, the hidden state is the output of the RNN at the previous time step. That hidden state is then responsible for two things:\n",
    "\n",
    "- Having the right information for the output layer to predict the correct next token\n",
    "- Retaining memory of everything that happened in the sentence  \n",
    "\n",
    "In practice, RNNs are really bad at retaining memory of what happened much earlier in the sentence, which is the motivation to have another hidden state (called cell state) in the LSTM. The cell state will be responsible for keeping long short-term memory, while the hidden state will focus on the next token to predict. Let's take a closer look at how this is achieved and build an LSTM from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f48f3-f94d-4c75-8605-5bca12ad7a9f",
   "metadata": {},
   "source": [
    "#### Building an LSTM from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483529a-805a-49d4-b1ea-d776cf86e621",
   "metadata": {},
   "source": [
    "In terms of code, we can write the same steps like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ec02f-96d7-40ed-b147-bee1e9c58f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.forget_gate = nn.Linear(ni + nh, nh)\n",
    "        self.input_gate  = nn.Linear(ni + nh, nh)\n",
    "        self.cell_gate   = nn.Linear(ni + nh, nh)\n",
    "        self.output_gate = nn.Linear(ni + nh, nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        h = torch.cat([h, input], dim=1)\n",
    "        forget = torch.sigmoid(self.forget_gate(h))\n",
    "        c = c * forget\n",
    "        inp = torch.sigmoid(self.input_gate(h))\n",
    "        cell = torch.tanh(self.cell_gate(h))\n",
    "        c = c + inp * cell\n",
    "        out = torch.sigmoid(self.output_gate(h))\n",
    "        h = out * torch.tanh(c)\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c59399-403d-48ff-b666-aac50413a0ef",
   "metadata": {},
   "source": [
    "We can then refactor the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2bdd3-638f-4c3f-ae3a-ca81c5e846c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.ih = nn.Linear(ni,4*nh)\n",
    "        self.hh = nn.Linear(nh,4*nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        # One big multiplication for all the gates is better than 4 smaller ones\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
    "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "\n",
    "        c = (forgetgate*c) + (ingate*cellgate)\n",
    "        h = outgate * c.tanh()\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df58d82-161e-45bc-bc7b-63d300f636d8",
   "metadata": {},
   "source": [
    "Here we use the PyTorch chunk method to split our tensor into four pieces. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b6df3-ce16-4662-9b37-f832c8f71397",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(0,10); t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1ab61-1288-4986-8ac5-697ef8a9381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.chunk(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e6851-c0d8-4465-8b92-aa7db9931cc6",
   "metadata": {},
   "source": [
    "### Training a Language Model using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652494f-665f-49e1-aa87-c91e8e060bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel6(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90183ee8-89e0-4546-acf0-4cee5734ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel6(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69966244-11df-4aa9-9a93-836f9261d620",
   "metadata": {},
   "source": [
    "This gives better result than the multilayer RNN but there's still overfitting  which is a sign that a bit of regularization might help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a2d24-66ee-482a-931f-d2ab3949929b",
   "metadata": {},
   "source": [
    "### Regularizing an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae353a-a071-492d-808b-bb5f9305e565",
   "metadata": {},
   "source": [
    "Recurrent neural networks, in general, are hard to train, because of the problem of vanishing activations and gradients we saw before. Using LSTM (or GRU) cells makes training easier than with vanilla RNNs, but they are still very prone to overfitting. Overall, data augmentation for text data is currently not a well-explored space.\n",
    "\n",
    "However, there are other regularization techniques we can use instead to reduce overfitting, which were thoroughly studied for use with LSTMs. They include:  dropout, activation regularization, and temporal activation regularization which could allow an LSTM to beat state-of-the-art results that previously required much more complicated models. Such LSTM models that use these techniques are referred to as *AWD-LSTM*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9cd47-b224-4024-9a67-2eaf25551c8b",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a1448-8308-4ae5-bf84-7245c6567552",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique whose idea is to randomly change some activations to zero at training time. This makes sure all neurons actively work toward the output.  \n",
    "This is a full implementation of the dropout layer in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edfe013-8739-4801-8a92-7695d53b32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p): self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        mask = x.new(*x.shape).bernoulli_(1-p)\n",
    "        return x * mask.div_(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034844f3-2c77-409b-8c81-bebeff31abf2",
   "metadata": {},
   "source": [
    "In fastai, we can replace this class with nn.Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ba960-fdfa-4391-bc90-c573576373b8",
   "metadata": {},
   "source": [
    "#### Activation Regularization and Temporal Activation Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764709a-e854-4f85-a81f-a029b378e14a",
   "metadata": {},
   "source": [
    "Activation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very similar to weight decay. When applying weight decay, we add a small penalty to the loss that aims at making the weights as small as possible. For activation regularization, it's the final activations produced by the LSTM that we will try to make as small as possible, instead of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9ac8b-3a61-4546-9efe-c51cfa34c7b5",
   "metadata": {},
   "source": [
    "### Training a Weight-Tied Regularized LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e5f8c-6e52-4afd-a256-10cab9109009",
   "metadata": {},
   "source": [
    "Another useful trick we can add is *weight tying*. In a language model, the input embeddings represent a mapping from English words to activations, and the output hidden layer represents a mapping from activations to English words. We might expect, intuitively, that these mappings could be the same. We can represent this in PyTorch by assigning the same weight matrix to each of these layers:\n",
    "\n",
    "    self.h_o.weight = self.i_h.weight  \n",
    "    \n",
    "In LMModel7, we include these final tweaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c313d2f-25e8-4987-a1e8-830c6c24a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)\n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(out),raw,out\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2d8fc-5ef3-4417-87e3-37cef763ef5f",
   "metadata": {},
   "source": [
    "We can create a regularized Learner using the RNNRegularizer callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea024c-00a3-4e2b-a1e8-e047885c3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
    "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
    "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7ab15-4d03-4d4e-ac32-aefd8795aadd",
   "metadata": {},
   "source": [
    "A TextLearner automatically adds those two callbacks for us (with those values for alpha and beta as defaults), so we can simplify the preceding line to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298a4cc-2eca-448d-98c4-a3ab25572447",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
    "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a18192-f679-4216-aa25-548645353b52",
   "metadata": {},
   "source": [
    "We can then train the model, and add additional regularization by increasing the weight decay to `0.1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef039487-1b6d-467f-803b-7055f292f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
