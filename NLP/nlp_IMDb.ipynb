{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9a6b58-b33a-41e9-a645-b192d23d9561",
   "metadata": {},
   "source": [
    "# NLP with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ad72b-469b-4799-981b-fe2f0bd0e068",
   "metadata": {},
   "source": [
    "This notebook uses RNN to do text classification. We use the IMDB dataset to train a model that classifies movie reviews as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaab64a-3b82-4866-9f34-7372474384fe",
   "metadata": {},
   "source": [
    "## Tokenization  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb467d9-53b9-4ccc-bb7a-997fdf1f98ef",
   "metadata": {},
   "source": [
    "### Word Tokenization with fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb1398-9d75-40de-a0a4-c3fdf6d78177",
   "metadata": {},
   "source": [
    "Grabbing the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc253e-91b8-4cb6-90a9-49679b96d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3bf0d-3d7e-4da2-ae7d-1edab1a8799e",
   "metadata": {},
   "source": [
    "To get the text file in the path for tokenization using get_text_file.  \n",
    "We cna also pass the folders to restrict the search to a particular list of subfolders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf143-f788-430b-9831-8544d1ddad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dca851-b1a6-4bdd-a7c8-3f2c5b18257a",
   "metadata": {},
   "source": [
    "Grabbing the first file, open and read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021e0d3-ecc6-4c01-bba9-50e8f5dc1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ab163-3651-43ab-801e-36840bd97da8",
   "metadata": {},
   "source": [
    "Fastai uses a library called spaCy for tokenization. As we are doing word tokenization, we will have to specify that.  \n",
    "Also, we use fastai's coll_repr(collection, n) function to display the results. This displays the first n items of collection.  \n",
    "We have to pass txt as a list to our tokenizer(spacy) as it only takes a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e772a7-d4e5-4ecc-b120-355fc6f7476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d97af-991b-4eed-8874-36c2fcb48a33",
   "metadata": {},
   "source": [
    "spaCy separates \".\" when it's being used to terminate a sentence but not in an acroynm or number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaec474-d2bd-4d12-a5bd-d52a039339c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e9bb2-5e71-4bcf-a59c-650d4b4550f9",
   "metadata": {},
   "source": [
    "Tokenizer class by fastai adds some additional functionality to the tokenization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15268910-d258-4a4e-a028-48e3ca2dc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2d96e-b64c-446f-b70c-f89dbd6127ac",
   "metadata": {},
   "source": [
    "This allows us to lowercase everything so that the embedding matrix can only work with lowercase text. \n",
    "Words that begin with a capital letter have a special token 'xxmaj' while the beginning of a stream is indicated by 'xxbos'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383b81d-b2b2-43a3-881b-b562a7899e66",
   "metadata": {},
   "source": [
    "Other special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287f246-4dec-4305-9ff6-3a12adc08fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c373dbc-4159-4722-abba-642ca14a7716",
   "metadata": {},
   "source": [
    "xxrep: replaces any character repeated 3 or more times with a special token with a special token for repetition (xxrep), the number of times it's repeated, then the character.  \n",
    "xxup: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb9fce-bb00-48f3-8638-475a06c21b14",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cef4ee-1627-44d9-9f0c-496f17090f95",
   "metadata": {},
   "source": [
    "This method follows the following steps:  \n",
    "1) Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.\n",
    "2) Tokenize the corpus using this vocab of subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff8b90-909d-41cb-afea-0a68b546328e",
   "metadata": {},
   "source": [
    "Let's look at an example.  \n",
    "For our corpus, we'll use the first 2,000 movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1143b-9250-4308-a1ae-1bb7b3130650",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in [:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0442e-a9c1-476d-b1f5-895975f976fe",
   "metadata": {},
   "source": [
    "Now we can use setup(), which is a special fastai method that is used to train our Tokenizer to find common sequences of characters to create the vocab.  \n",
    "We'll create a function that takes a certain size of a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc614b-71a2-4a86-962d-91ac1dd39bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubWordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp[txts]))[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34629ff-00ca-4680-900f-767ab41ca6d0",
   "metadata": {},
   "source": [
    "Trying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc68fa9-eff4-4984-94ee-32703522356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120bac0-71c7-4deb-a4a7-b54554bd5033",
   "metadata": {},
   "source": [
    "The special character ‚ñÅ represents a space character in the original text when using fastai's subword tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e97f5-8971-4c9c-b86f-08c188d4f7da",
   "metadata": {},
   "source": [
    "For small vocabs, each token will represent fewer characters, and it will take more tokens to represent a sentence.  \n",
    "For larger vocabs, most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa542f13-b439-40cc-ba38-ebdf3c4f1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b04a64-bc0f-4116-bb9f-a513e53add6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceee541-83df-4b2b-8911-d9d48c8f8ac1",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233dcdb-b6ac-4729-a434-8c40eec8cd6b",
   "metadata": {},
   "source": [
    "Numericalization is the process of mapping tokens to integers. It involves:  \n",
    "1) Make a list of all possible levels of that categorical variable (the vocab).  \n",
    "2) Replace each level with its index in the vocab. \n",
    "\n",
    "We'll use the word tokenized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e208d-b21f-4c06-a439-0621d90f9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405e410-db46-4d0d-b43c-303d832a0122",
   "metadata": {},
   "source": [
    "In order to numericalize, we first have to call setup() that creates a vocab.  \n",
    "Since tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ce388-7986-4990-a193-39b2123856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6064c-e81b-473c-90f5-8e7db61d4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578c52a-45b8-4279-b06c-6bc248bad863",
   "metadata": {},
   "source": [
    "Our special rules tokens appear first, and then every word appears once, in frequency order\n",
    "The default vocab size is a maximum of 60000 and thats's the size of the embedding matrix by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303ddb5-0fce-478d-8ad5-ebd316b0fc0c",
   "metadata": {},
   "source": [
    "Once we've created our Numericalize object, we can use it as if it were a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea5d70-e96f-4824-a9f9-fcd006a5723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f96e49-565d-4da8-a528-60d7153c1926",
   "metadata": {},
   "source": [
    "This will replace our words with tensors of integer.  \n",
    "We can check that they map back to the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5e0ae-e725-4dae-97f3-53806c2d4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73884c3a-9020-47af-91bd-9caa05d54d2e",
   "metadata": {},
   "source": [
    "## Putting our Text into Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01bfe1-a45a-40f0-957c-6e3f47e8fc0a",
   "metadata": {},
   "source": [
    "The first step is to transform the individual texts into a stream by concatenating them together. At the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside them, or the texts would not make sense anymore!). We then cut this stream into a certain number of batches (which is our batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd7949-81ea-4216-90f0-d673205577af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums200 = toks200.map(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd518b4-7b62-4684-a67a-50d16fad6f7d",
   "metadata": {},
   "source": [
    "We pass LMDataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c6f2e-b255-4cdc-93d1-cad9d04b495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(nums200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059036a2-0148-4645-bbcc-913e70c68d4f",
   "metadata": {},
   "source": [
    "Let's see if we get the expected results by grabbing the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9bfaa-e2d0-4127-81a5-bb13e1261a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc20cf-9ab6-41be-943d-7d3e5d89c0da",
   "metadata": {},
   "source": [
    "Looking at the first row of the independent variable, which should be the start of the first text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c226e56-73fe-427f-a5c0-9a4a1f00f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12c58a-e9a0-43ea-ae3e-e9343c697513",
   "metadata": {},
   "source": [
    "The dependent variable is the same thing offset by one token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d73d58-a3c4-49d3-b58b-e46626fb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba07fd-29d8-4469-842a-5323837172d8",
   "metadata": {},
   "source": [
    "## Train a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc663c-4594-4235-a402-c0058105e214",
   "metadata": {},
   "source": [
    "### Language Model Using DataBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a2d20-a1cd-4488-a0c2-8ad22f28c462",
   "metadata": {},
   "source": [
    "Here's how we use TextBlock to create a language model, using fastai's defaults. TextBlock handles both Tokenization and Numericalization atomatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d75d6d-a35d-4c4a-8144-c520851c86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks = TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items = get_imdb, splitter = RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b88c5-d945-4aa5-94ac-74bccb96db69",
   "metadata": {},
   "source": [
    "We set a batch size of 128 and sequence length of 80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d35b9-df6f-477b-aada-90e5ee5478be",
   "metadata": {},
   "source": [
    "We call show_batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d55f1-dabd-4df5-8639-275bfd570d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1f36f-3436-476d-9ccb-cae686174c6f",
   "metadata": {},
   "source": [
    "### Fine tuning the language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a566b5-80e4-4a68-a664-3d0a0b428c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
