{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9a6b58-b33a-41e9-a645-b192d23d9561",
   "metadata": {},
   "source": [
    "# NLP with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ad72b-469b-4799-981b-fe2f0bd0e068",
   "metadata": {},
   "source": [
    "This notebook uses RNN to do text classification. We use the IMDB dataset to train a model that classifies movie reviews as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaab64a-3b82-4866-9f34-7372474384fe",
   "metadata": {},
   "source": [
    "## Tokenization  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb467d9-53b9-4ccc-bb7a-997fdf1f98ef",
   "metadata": {},
   "source": [
    "### Word Tokenization with fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb1398-9d75-40de-a0a4-c3fdf6d78177",
   "metadata": {},
   "source": [
    "Grabbing the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc253e-91b8-4cb6-90a9-49679b96d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3bf0d-3d7e-4da2-ae7d-1edab1a8799e",
   "metadata": {},
   "source": [
    "To get the text file in the path for tokenization using get_text_file.  \n",
    "We cna also pass the folders to restrict the search to a particular list of subfolders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf143-f788-430b-9831-8544d1ddad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dca851-b1a6-4bdd-a7c8-3f2c5b18257a",
   "metadata": {},
   "source": [
    "Grabbing the first file, open and read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021e0d3-ecc6-4c01-bba9-50e8f5dc1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ab163-3651-43ab-801e-36840bd97da8",
   "metadata": {},
   "source": [
    "Fastai uses a library called spaCy for tokenization. As we are doing word tokenization, we will have to specify that.  \n",
    "Also, we use fastai's coll_repr(collection, n) function to display the results. This displays the first n items of collection.  \n",
    "We have to pass txt as a list to our tokenizer(spacy) as it only takes a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e772a7-d4e5-4ecc-b120-355fc6f7476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d97af-991b-4eed-8874-36c2fcb48a33",
   "metadata": {},
   "source": [
    "spaCy separates \".\" when it's being used to terminate a sentence but not in an acroynm or number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaec474-d2bd-4d12-a5bd-d52a039339c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e9bb2-5e71-4bcf-a59c-650d4b4550f9",
   "metadata": {},
   "source": [
    "Tokenizer class by fastai adds some additional functionality to the tokenization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15268910-d258-4a4e-a028-48e3ca2dc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2d96e-b64c-446f-b70c-f89dbd6127ac",
   "metadata": {},
   "source": [
    "This allows us to lowercase everything so that the embedding matrix can only work with lowercase text. \n",
    "Words that begin with a capital letter have a special token 'xxmaj' while the beginning of a stream is indicated by 'xxbos'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383b81d-b2b2-43a3-881b-b562a7899e66",
   "metadata": {},
   "source": [
    "Other special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287f246-4dec-4305-9ff6-3a12adc08fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c373dbc-4159-4722-abba-642ca14a7716",
   "metadata": {},
   "source": [
    "xxrep: replaces any character repeated 3 or more times with a special token with a special token for repetition (xxrep), the number of times it's repeated, then the character.  \n",
    "xxup: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb9fce-bb00-48f3-8638-475a06c21b14",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cef4ee-1627-44d9-9f0c-496f17090f95",
   "metadata": {},
   "source": [
    "This method follows the following steps:  \n",
    "1) Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.\n",
    "2) Tokenize the corpus using this vocab of subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff8b90-909d-41cb-afea-0a68b546328e",
   "metadata": {},
   "source": [
    "Let's look at an example.  \n",
    "For our corpus, we'll use the first 2,000 movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1143b-9250-4308-a1ae-1bb7b3130650",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in [:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0442e-a9c1-476d-b1f5-895975f976fe",
   "metadata": {},
   "source": [
    "Now we can use setup(), which is a special fastai method that is used to train our Tokenizer to find common sequences of characters to create the vocab.  \n",
    "We'll create a function that takes a certain size of a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc614b-71a2-4a86-962d-91ac1dd39bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubWordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp[txts]))[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34629ff-00ca-4680-900f-767ab41ca6d0",
   "metadata": {},
   "source": [
    "Trying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc68fa9-eff4-4984-94ee-32703522356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120bac0-71c7-4deb-a4a7-b54554bd5033",
   "metadata": {},
   "source": [
    "The special character ‚ñÅ represents a space character in the original text when using fastai's subword tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e97f5-8971-4c9c-b86f-08c188d4f7da",
   "metadata": {},
   "source": [
    "For small vocabs, each token will represent fewer characters, and it will take more tokens to represent a sentence.  \n",
    "For larger vocabs, most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa542f13-b439-40cc-ba38-ebdf3c4f1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b04a64-bc0f-4116-bb9f-a513e53add6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceee541-83df-4b2b-8911-d9d48c8f8ac1",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233dcdb-b6ac-4729-a434-8c40eec8cd6b",
   "metadata": {},
   "source": [
    "Numericalization is the process of mapping tokens to integers. It involves:  \n",
    "1) Make a list of all possible levels of that categorical variable (the vocab).  \n",
    "2) Replace each level with its index in the vocab. \n",
    "\n",
    "We'll use the word tokenized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e208d-b21f-4c06-a439-0621d90f9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405e410-db46-4d0d-b43c-303d832a0122",
   "metadata": {},
   "source": [
    "In order to numericalize, we first have to call setup() that creates a vocab.  \n",
    "Since tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ce388-7986-4990-a193-39b2123856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6064c-e81b-473c-90f5-8e7db61d4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578c52a-45b8-4279-b06c-6bc248bad863",
   "metadata": {},
   "source": [
    "Our special rules tokens appear first, and then every word appears once, in frequency order\n",
    "The default vocab size is a maximum of 60000 and thats's the size of the embedding matrix by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303ddb5-0fce-478d-8ad5-ebd316b0fc0c",
   "metadata": {},
   "source": [
    "Once we've created our Numericalize object, we can use it as if it were a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea5d70-e96f-4824-a9f9-fcd006a5723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f96e49-565d-4da8-a528-60d7153c1926",
   "metadata": {},
   "source": [
    "This will replace our words with tensors of integer.  \n",
    "We can check that they map back to the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5e0ae-e725-4dae-97f3-53806c2d4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73884c3a-9020-47af-91bd-9caa05d54d2e",
   "metadata": {},
   "source": [
    "## Putting our Text into Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01bfe1-a45a-40f0-957c-6e3f47e8fc0a",
   "metadata": {},
   "source": [
    "The first step is to transform the individual texts into a stream by concatenating them together. At the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside them, or the texts would not make sense anymore!). We then cut this stream into a certain number of batches (which is our batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd7949-81ea-4216-90f0-d673205577af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums200 = toks200.map(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd518b4-7b62-4684-a67a-50d16fad6f7d",
   "metadata": {},
   "source": [
    "We pass LMDataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c6f2e-b255-4cdc-93d1-cad9d04b495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(nums200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059036a2-0148-4645-bbcc-913e70c68d4f",
   "metadata": {},
   "source": [
    "Let's see if we get the expected results by grabbing the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9bfaa-e2d0-4127-81a5-bb13e1261a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc20cf-9ab6-41be-943d-7d3e5d89c0da",
   "metadata": {},
   "source": [
    "Looking at the first row of the independent variable, which should be the start of the first text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c226e56-73fe-427f-a5c0-9a4a1f00f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12c58a-e9a0-43ea-ae3e-e9343c697513",
   "metadata": {},
   "source": [
    "The dependent variable is the same thing offset by one token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d73d58-a3c4-49d3-b58b-e46626fb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba07fd-29d8-4469-842a-5323837172d8",
   "metadata": {},
   "source": [
    "## Train a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc663c-4594-4235-a402-c0058105e214",
   "metadata": {},
   "source": [
    "### Language Model Using DataBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a2d20-a1cd-4488-a0c2-8ad22f28c462",
   "metadata": {},
   "source": [
    "Here's how we use TextBlock to create a language model, using fastai's defaults. TextBlock handles both Tokenization and Numericalization atomatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4817bf-609e-43dd-a97e-69df2c958c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear gpu cache memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d75d6d-a35d-4c4a-8144-c520851c86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks = TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items = get_imdb, splitter = RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=60, seq_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b88c5-d945-4aa5-94ac-74bccb96db69",
   "metadata": {},
   "source": [
    "We set a batch size of 128 and sequence length of 80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d35b9-df6f-477b-aada-90e5ee5478be",
   "metadata": {},
   "source": [
    "We call show_batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d55f1-dabd-4df5-8639-275bfd570d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1f36f-3436-476d-9ccb-cae686174c6f",
   "metadata": {},
   "source": [
    "### Fine tuning the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7daacc3-3499-4a34-ad55-d890dae14155",
   "metadata": {},
   "source": [
    "We'll create a learner which is going to learn to predict the next word of a movie review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925dfd3-5950-4483-852c-8c3476f89508",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
    "    metrics = [accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58460f-0a16-42b1-8266-437cec7c2425",
   "metadata": {},
   "source": [
    "Call fit_one_cycle():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eff7cb-5afc-4a54-9b97-2cc39687e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83863b-0848-4ffd-a1a2-503266483875",
   "metadata": {},
   "source": [
    "### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a3504-0ce2-4d34-80e3-929ef6ed3240",
   "metadata": {},
   "source": [
    "We can save the state of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76022b69-62ee-4b8d-b008-b5a62450423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('1epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ab94f-6ce7-42df-ba38-032c0fd5391d",
   "metadata": {},
   "source": [
    "We can then load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa521a25-d497-4664-a583-4bc9cd4ff8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd8f4e-6e8f-4b1f-91cb-fcfdbb95bdec",
   "metadata": {},
   "source": [
    "We can continue fine-tuning the model after unfreezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d1b49-4bdd-4612-8849-7af32fc04ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ed9d6-c1ab-4763-9d9c-b416166efccb",
   "metadata": {},
   "source": [
    "We save the encoder model as we won't be saving the final layers(task specific):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197cfb-d1fa-4eef-9184-98e1baea1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc09c03-342a-4991-8ff6-64ee40b2d6a2",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3fa2d-6ac9-49c5-b081-190fb4b85513",
   "metadata": {},
   "source": [
    "We can try something different(text generation of movie reviews as):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526790a0-9f80-4e2c-8332-886f5fbec365",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"I like this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature = 0.75) \n",
    "         for _ in (N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455e6d1-a0db-45a9-adf7-7b4c0cff0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff084e0b-dbc2-4ac4-9361-50dbf1327f4e",
   "metadata": {},
   "source": [
    "Not bad for a model with 35% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cea454-29c0-4ff5-a0c7-4bd2c0f96250",
   "metadata": {},
   "source": [
    "### Creating the Classifier Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df8c2b-337c-4de8-9fbc-7d3ed1e60888",
   "metadata": {},
   "source": [
    "As we want a model that classifies reviews as positive or negative, we need to fine tune the model to that specific task.  \n",
    "Here's the dataloader for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542d28c-33cf-45ab-8f7d-90d59f6c8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks = (TextBlock.from_folder(path, vocab=dls_lm.vocacab), CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items = partial(get_text_files, folders = ['train', 'test']),\n",
    "    splitter = GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=60, seq_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e8cde-16a3-4f45-a3e9-c3528d6d7ae2",
   "metadata": {},
   "source": [
    "Now, we can show batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177d4af-e386-4a04-a5d5-d5f7808cd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593d6fd-2b03-4b9e-bf39-10be87a03c5f",
   "metadata": {},
   "source": [
    "For language models, we were able to split the data into strings of equal length to create a batch size and load it to learner. we cannot do this for the movie reviews. We need to load the whole movie review to be able to classify it. As we are using a batch size of 60 and movie reviews often are about 3000 words long, this will be a problem to fit in GPU memory. We can split them to avoid memory error.\n",
    "Let's see with an example, by trying to create a mini-batch containing the first 10 documents. First we'll numericalize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bd3a1-7638-49cd-809d-e42c53c503ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp=toks200[:10].map(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79422561-ade5-4cb7-9dd2-9232d10a099c",
   "metadata": {},
   "source": [
    "Let's now look at how many tokens each of these 10 movie reviews have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0ab61-5fa9-491b-90c6-70e7acf7e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp.map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93363edf-32e3-4b46-8df2-f3860783a063",
   "metadata": {},
   "source": [
    "From the output, we get different sizes and therefore we can't split them into sequences of 100. We need to do 'padding'(expand the shortest texts to make them all the same size) so as to be able to split into equal sequence length. As we have used the data block API(that has TextBlock and is_lm=False), we dont have to do it manually.  \n",
    "\n",
    "We can now create a model to classify our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027cb7c-4004-42d8-abb2-dfc7fa6a5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1a6a1-cc3a-4f5e-9845-d97a261b84ca",
   "metadata": {},
   "source": [
    "We can now load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff02ec-b4ea-4288-a92a-46de4194e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_encooder('finetuned')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8718f2e5-759c-4f2e-b4f6-4acf694bf15b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bfd58-3316-4f08-902b-1b30b198e20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
