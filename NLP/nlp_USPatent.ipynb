{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fc4f7f-03c3-4b55-ac5d-e98be4618b8e",
   "metadata": {},
   "source": [
    "# NLP with HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608fd0e-af06-488f-ae66-e82cf4092fd4",
   "metadata": {},
   "source": [
    "For the Kaggle [US Patent Phrase to Phrase Matching](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/), we are tasked with comparing two words or short phrases, and scoring them based on whether they're similar or not, based on which patent class they were used in. With a score of 1 it is considered that the two inputs have identical meaning, and 0 means they have totally different meaning. For instance, abatement and eliminating process have a score of 0.5, meaning they're somewhat similar, but not identical.  \n",
    "It turns out that this can be represented as a classification problem. How? By representing the question like this:\n",
    "\n",
    "For the following text...: \"TEXT1: abatement; TEXT2: eliminating process\" ...chose a category of meaning similarity: \"Different; Similar; Identical\".\n",
    "\n",
    "In this notebook we'll see how to solve the Patent Phrase Matching problem by treating it as a classification task, by representing it in a very similar way to that shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675401be-150e-48e2-9863-b8badbccc918",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881019a1-ed50-4132-a2eb-58607626c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a631d-cffb-45ae-ac92-dc3c59e82316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed kaggle\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5376f-edda-448d-b119-2a4ac10f6b4f",
   "metadata": {},
   "source": [
    "We'll create Kaggle API token and use it to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d566b3-a571-45c8-ae88-3a9f0db511f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = '{username:\"maureenwamuyumugo\", key:\"6937e0396ac38e5f307a2fccbf466138}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96845a-8d1f-4c1a-87a1-8587ac892baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cred_path = Path\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462b4f5-d613-4eca-93cc-91b5b0733471",
   "metadata": {},
   "source": [
    "Now you can download datasets from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a23054-d0da-47f1-9329-a0c4d241f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('us-patent-phrase-to-phrase-matching')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a01852-5f52-4aa8-89a7-fc40f740626a",
   "metadata": {},
   "source": [
    "And use the Kaggle API to download the dataset to that path, and extract it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aabe59-6085-4037-99ab-246e09df1741",
   "metadata": {},
   "source": [
    "Now we can check what's in path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c74db6-6c6a-4f60-905c-251eb19a3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dd93c-7ceb-4c71-aa6a-2e8b7b163903",
   "metadata": {},
   "source": [
    "These are CSv files and we can use pandas to read them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed050ad-a65e-4901-91e3-282793357be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544a82d-f6df-4730-ae8b-20891c4d47fa",
   "metadata": {},
   "source": [
    "Let's set a path to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef975e6-4c13-4c3f-87c8-25f91d475a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f79df-2f08-462e-ac6f-0586714756f7",
   "metadata": {},
   "source": [
    "This creates a DataFrame, which is a table of columns, a bit like a database table.  \n",
    "To view the first and last 5 rows, and row count of a DataFrame, just type its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767d6b4-89bb-4216-9a41-7eb008cceba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c55b7-11d9-49c7-9115-122ca4c65d3f",
   "metadata": {},
   "source": [
    "It's important to carefully read the dataset description to understand how each of these columns is used.  \n",
    ".describe() method is also important for understanding a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503598b-b894-49b6-a391-bae786b904e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2277e-f1a8-4c53-a6c4-110d433caa90",
   "metadata": {},
   "source": [
    "To create a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ab7d7-0509-4836-9e77-ec5a84989ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42675119-ce04-4be6-8205-240dc1b5a77b",
   "metadata": {},
   "source": [
    "To get the first few rows, use head():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b53e1a-0f53-43ea-8de3-c1eaea3f4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5800a71-0ed8-4c46-9154-b61621ad47fb",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e55837-e205-4b02-9e76-465f5c02e7e7",
   "metadata": {},
   "source": [
    "We'll turn our pandas DataFrame into a HuggingFace dataset as Transformers uses a Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e42cd-7030-4f1e-b2a4-9dac0ad95563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f3389-20d4-4a16-a3ab-e9e93171a61a",
   "metadata": {},
   "source": [
    "Here's how it's displayed in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d42b9d-d899-4768-b8f7-b9d90b5bbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfde79-565b-4376-911c-6b2dd5a8f9d6",
   "metadata": {},
   "source": [
    "But we can't pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n",
    "\n",
    "- Tokenization: Split each text up into words (or actually, as we'll see, into tokens)\n",
    "- Numericalization: Convert each word (or token) into a number.  \n",
    "\n",
    "Before Tokenization, you have to decide what model to use. HuggingFace has good models that work for a lot of things most of the time like `deberta-v3`. We'll start with small because its faster to train and we can do more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821abff-2f92-409c-a517-dd38d7808212",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc4a34-4db0-44fb-a1d5-ac989a0f5006",
   "metadata": {},
   "source": [
    "To tell the transformer to tokenize the same way the model was built to tokenize, we use `AutoTokenizer`.  \n",
    "AutoTokenizer -> dictionary that creates a tokenizer appropriate for a given model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46ec59-02bf-46f1-a24f-26579358f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39424687-85ad-4536-a6d6-068a6d5b1591",
   "metadata": {},
   "source": [
    "Now we can take the tokenizer and pass a string to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528177f1-2e5d-4348-95be-9b8f4b4da0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b5d08-108f-4d4e-9f81-425131c07f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.tekenize(\"A platypus is an ornithorhynchus anatinus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc91fa-6f73-4b1e-ba4f-60c092599031",
   "metadata": {},
   "source": [
    "This splits the string into tokens. All the tokens have to be in the vocabulary, the list of unique tokens that was created when this particular pretrained model was first trained. The `_` represents the start of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac56104-5b96-4e5b-9faf-81f112d1cb8a",
   "metadata": {},
   "source": [
    "Here's a simple function that takes a document, grabs its input and tokenizes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96b74f-9f85-4e6d-8c8b-0058cda9157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0483d-7d80-4183-876e-fd5a7112c0ed",
   "metadata": {},
   "source": [
    "To run this quickly in parallel on every row in our dataset, use map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab298bb-fcb2-4a26-9c98-814b629e00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425c9c8-46e9-42fb-bb56-b4acdd743332",
   "metadata": {},
   "source": [
    "`batched=True` allows it to go through the tokenizer libary a bunch at a time.  \n",
    "This adds a new item to our dataset called `input_ids`. For instance, here is the input and IDs for the first row of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7b43f-016b-4bec-b54a-2d2aeb39393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = tok_ds[0]\n",
    "row['input'], row['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8c20d-077c-4790-b6d2-4e893f2b8ac4",
   "metadata": {},
   "source": [
    "So now we have turned the strings from tokens to numbers. This is called `Numericalization`.  \n",
    "\n",
    "We can look them up like this, for instance to find the token for the word \"of\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc29596-bb0b-40c1-8531-2b886fa89fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.vocab['▁of']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feecc44-2181-4a60-b8da-295488d9e4fc",
   "metadata": {},
   "source": [
    "Its 265 and in the previous output we had 265.  \n",
    "\n",
    "HuggingFace transformers expects that your target is a column called `labels`, so we can rename our token-dataset score column to 'label':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d000e-6d80-400c-b12c-f7e7900384a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6e9ef-b64d-43b7-8578-7d83762be0a8",
   "metadata": {},
   "source": [
    "In ML, its important to have a separate training, validation and test dataset. Test and validation set are all about identifying and controlling overfitting.\n",
    "\n",
    "### Test and Validation Set\n",
    "You may have noticed that our directory contained another file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e4bef-513f-4751-a3c9-2c4399121c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(path/'test.csv')\n",
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87074de-8e6a-46af-a30e-1d1b6d266c56",
   "metadata": {},
   "source": [
    "This is the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd455eed-70ad-4ca2-bf05-68633c2f68ee",
   "metadata": {},
   "source": [
    "Transformers uses a DatasetDict for holding your training and validation sets. To create one that contains 25% of our data for the validation set, and 75% for the training set, use train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4e629-f1d5-43a3-b92d-23c3c2a2a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75bcca-2da2-4e9b-8581-d73bf1a28b00",
   "metadata": {},
   "source": [
    "The test set is yet another dataset that's held out from training. But it's held out from reporting metrics too! The accuracy of your model on the test set is only ever checked after you've completed your entire training process, including trying different models, training methods, data processing, etc.  \n",
    "We'll use eval as our name for the test set, to avoid confusion with the test dataset that was created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1bc8b-1774-4183-8686-856f57294fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ace63-f2a1-4cdd-bca0-a17a8337a010",
   "metadata": {},
   "source": [
    "### Metrics and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeaa848-6052-4b21-ba37-22a03920d2ca",
   "metadata": {},
   "source": [
    "When we're training a model, there will be one or more metrics that we're interested in maximising or minimising. These are the measurements that should, hopefully, represent how well our model will works for us.  \n",
    "In Kaggle, however, it's very straightforward to know what metric to use: Kaggle will tell you! According to this competition's [evaluation page](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/overview/evaluation), \"submissions are evaluated on the `Pearson correlation coefficient` between the predicted and actual similarity scores.\" This coefficient is usually abbreviated using the single letter `r`. It is the most widely used measure of the degree of relationship between two variables.  \n",
    "r can vary between -1, which means perfect inverse correlation(i.e predicted exactly the wrong answer), and +1, which means perfect positive correlation(i.e predicted exactly the right answer). The mathematical formula for it is much less important than getting a good intuition for what the different values look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f50bf0-27a8-4617-8101-6d18bb741354",
   "metadata": {},
   "source": [
    "Transformers expects metrics to be returned as a `dict`, since that way the trainer knows what label to use, so let's create a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df20a3a-26ab-4d07-9a61-90cc4567f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491d99c-ed5e-4693-805b-267243ceec47",
   "metadata": {},
   "source": [
    "## Training our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b5ea6-2173-41d8-955a-c8a44e680954",
   "metadata": {},
   "source": [
    "Fastai uses `Learner`, HuggingFace transformer's is called `Trainer`. So let's get that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc832efa-84be-44b4-a506-6b05987590a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6b739-1b53-4893-b4e2-352bc7d6bfd0",
   "metadata": {},
   "source": [
    "We pick a batch size that fits our GPU, and small number of epochs so we can run experiments quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008708e1-cda3-4204-8085-669501e2fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278def2-3763-4c6d-b7bf-a86007fe4bce",
   "metadata": {},
   "source": [
    "The most important hyperparameter is the learning rate. fastai provides a learning rate finder to help you figure this out, but Transformers doesn't, so you'll just have to use trial and error. The idea is to find the largest value you can, but which doesn't result in training failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4a3ed-ea37-4324-af73-d837b3b2847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82804b2e-2478-4f49-9a8e-991592327b5c",
   "metadata": {},
   "source": [
    "Transformers uses the `TrainingArguments` class to set up arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abac4a1-8252-4ff2-9a2d-bb19efd31694",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9686dc-1a64-4145-b51e-e634034a3d8d",
   "metadata": {},
   "source": [
    "We can now create our model, and `Trainer`, which is a class which combines the data and model together (just like Learner in fastai):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f113452-c56d-4100-aa06-fe0021c68510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz, compute_metrics=corr_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ccc6b-f6cf-46fd-96ae-2c640f7ae62e",
   "metadata": {},
   "source": [
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6a70e-8772-4024-9bf5-749f2440e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a80d0d-180a-42d2-9f2c-0c33535016b8",
   "metadata": {},
   "source": [
    "We get a Pearson value above 0.8 which is pretty good.  \n",
    "Let's get some predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b940c-9c5f-40b7-8635-ed6d957e5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c2278-cdcd-46d9-80f6-95e0803d0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "Look out - some of our predictions are <0, or >1! This once again shows the value of remember to actually look at your data. Let's fix those out-of-bounds predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8df055-c37e-488f-9a35-f382f98d74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.clip(preds, 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
