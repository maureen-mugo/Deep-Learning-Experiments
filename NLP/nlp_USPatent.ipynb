{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fc4f7f-03c3-4b55-ac5d-e98be4618b8e",
   "metadata": {},
   "source": [
    "# NLP with HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608fd0e-af06-488f-ae66-e82cf4092fd4",
   "metadata": {},
   "source": [
    "For the Kaggle [US Patent Phrase to Phrase Matching](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/), we are tasked with comparing two words or short phrases, and scoring them based on whether they're similar or not, based on which patent class they were used in. With a score of 1 it is considered that the two inputs have identical meaning, and 0 means they have totally different meaning. For instance, abatement and eliminating process have a score of 0.5, meaning they're somewhat similar, but not identical.  \n",
    "It turns out that this can be represented as a classification problem. How? By representing the question like this:\n",
    "\n",
    "For the following text...: \"TEXT1: abatement; TEXT2: eliminating process\" ...chose a category of meaning similarity: \"Different; Similar; Identical\".\n",
    "\n",
    "In this notebook we'll see how to solve the Patent Phrase Matching problem by treating it as a classification task, by representing it in a very similar way to that shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675401be-150e-48e2-9863-b8badbccc918",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881019a1-ed50-4132-a2eb-58607626c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a631d-cffb-45ae-ac92-dc3c59e82316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed kaggle\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5376f-edda-448d-b119-2a4ac10f6b4f",
   "metadata": {},
   "source": [
    "We'll create Kaggle API token and use it to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d566b3-a571-45c8-ae88-3a9f0db511f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = '{username:\"maureenwamuyumugo\", key:\"6937e0396ac38e5f307a2fccbf466138}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96845a-8d1f-4c1a-87a1-8587ac892baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cred_path = Path\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462b4f5-d613-4eca-93cc-91b5b0733471",
   "metadata": {},
   "source": [
    "Now you can download datasets from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a23054-d0da-47f1-9329-a0c4d241f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('us-patent-phrase-to-phrase-matching')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a01852-5f52-4aa8-89a7-fc40f740626a",
   "metadata": {},
   "source": [
    "And use the Kaggle API to download the dataset to that path, and extract it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aabe59-6085-4037-99ab-246e09df1741",
   "metadata": {},
   "source": [
    "Now we can check what's in path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c74db6-6c6a-4f60-905c-251eb19a3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dd93c-7ceb-4c71-aa6a-2e8b7b163903",
   "metadata": {},
   "source": [
    "These are CSv files and we can use pandas to read them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed050ad-a65e-4901-91e3-282793357be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544a82d-f6df-4730-ae8b-20891c4d47fa",
   "metadata": {},
   "source": [
    "Let's set a path to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef975e6-4c13-4c3f-87c8-25f91d475a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f79df-2f08-462e-ac6f-0586714756f7",
   "metadata": {},
   "source": [
    "This creates a DataFrame, which is a table of columns, a bit like a database table.  \n",
    "To view the first and last 5 rows, and row count of a DataFrame, just type its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767d6b4-89bb-4216-9a41-7eb008cceba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c55b7-11d9-49c7-9115-122ca4c65d3f",
   "metadata": {},
   "source": [
    "It's important to carefully read the dataset description to understand how each of these columns is used.  \n",
    ".describe() method is also important for understanding a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503598b-b894-49b6-a391-bae786b904e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2277e-f1a8-4c53-a6c4-110d433caa90",
   "metadata": {},
   "source": [
    "To create a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ab7d7-0509-4836-9e77-ec5a84989ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42675119-ce04-4be6-8205-240dc1b5a77b",
   "metadata": {},
   "source": [
    "To get the first few rows, use head():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b53e1a-0f53-43ea-8de3-c1eaea3f4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5800a71-0ed8-4c46-9154-b61621ad47fb",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e55837-e205-4b02-9e76-465f5c02e7e7",
   "metadata": {},
   "source": [
    "We'll turn our pandas DataFrame into a HuggingFace dataset as Transformers uses a Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e42cd-7030-4f1e-b2a4-9dac0ad95563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f3389-20d4-4a16-a3ab-e9e93171a61a",
   "metadata": {},
   "source": [
    "Here's how it's displayed in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d42b9d-d899-4768-b8f7-b9d90b5bbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfde79-565b-4376-911c-6b2dd5a8f9d6",
   "metadata": {},
   "source": [
    "But we can't pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n",
    "\n",
    "- Tokenization: Split each text up into words (or actually, as we'll see, into tokens)\n",
    "- Numericalization: Convert each word (or token) into a number.  \n",
    "\n",
    "Before Tokenization, you have to decide what model to use. HuggingFace has good models that work for a lot of things most of the time like `deberta-v3`. We'll start with small because its faster to train and we can do more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821abff-2f92-409c-a517-dd38d7808212",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc4a34-4db0-44fb-a1d5-ac989a0f5006",
   "metadata": {},
   "source": [
    "To tell the transformer to tokenize the same way the model was built to tokenize, we use `AutoTokenizer`.  \n",
    "AutoTokenizer -> dictionary that creates a tokenizer appropriate for a given model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46ec59-02bf-46f1-a24f-26579358f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39424687-85ad-4536-a6d6-068a6d5b1591",
   "metadata": {},
   "source": [
    "Now we can take the tokenizer and pass a string to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528177f1-2e5d-4348-95be-9b8f4b4da0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
